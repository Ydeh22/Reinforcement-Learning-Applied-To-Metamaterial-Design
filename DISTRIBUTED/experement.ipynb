{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributedDDPG import DDPGAgent, Episodes\n",
    "import ray\n",
    "import time\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from models import Actor, Critic\n",
    "import copy\n",
    "from env import DistributedTSCSEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'env_config': {\n",
    "        'nCyl': 4,\n",
    "        'k0amax': 0.45,\n",
    "        'k0amin': 0.35,\n",
    "        'nFreq': 11,\n",
    "        'actionRange': 0.2,\n",
    "        'episodeLength': 100},\n",
    "    'model': {\n",
    "        'actor_nHidden': 2,\n",
    "        'actor_hSize': 128,\n",
    "        'critic_nHidden': 8,\n",
    "        'critic_hSize': 128},\n",
    "    'num_workers': 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-01 21:06:30,442\tINFO services.py:1164 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '10.0.0.12',\n",
       " 'raylet_ip_address': '10.0.0.12',\n",
       " 'redis_address': '10.0.0.12:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2020-11-01_21-06-29_895930_2773572/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-11-01_21-06-29_895930_2773572/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2020-11-01_21-06-29_895930_2773572',\n",
       " 'metrics_export_port': 48653}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel env data generation: 18.601786375045776\n",
      "Parallel env data generation: 5.025837421417236\n",
      "Parallel env data generation: 4.2851402759552\n",
      "Parallel env data generation: 4.287029504776001\n",
      "Parallel env data generation: 4.35964822769165\n"
     ]
    }
   ],
   "source": [
    "agents = [DDPGAgent.remote(config) for _ in range(config['num_workers'])]\n",
    "data = []\n",
    "for _ in range(5):\n",
    "    futures = [agent.rollout_episode.remote(1.0) for agent in agents]\n",
    "    start = time.time()\n",
    "    data += ray.get(futures)\n",
    "    print(f'Parallel env data generation: {time.time() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_data = {\n",
    "    'states': [data[i]['states'] for i in range(len(data))],\n",
    "    'actions': [data[i]['actions'] for i in range(len(data))],\n",
    "    'rewards': [data[i]['rewards'] for i in range(len(data))],\n",
    "    'next_states': [data[i]['next_states'] for i in range(len(data))],\n",
    "    'dones': [data[i]['dones'] for i in range(len(data))]}\n",
    "\n",
    "for key in episode_data.keys():\n",
    "    episode_data[key] = np.concatenate(episode_data[key])\n",
    "    \n",
    "data = Episodes(episode_data)\n",
    "trainLoader = DataLoader(data, batch_size=32, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = 21\n",
    "action_space = 8\n",
    "\n",
    "actor = Actor(\n",
    "    observation_space,\n",
    "    2,\n",
    "    128,\n",
    "    action_space,\n",
    "    config['env_config']['actionRange']).cuda()\n",
    "\n",
    "critic = Critic(\n",
    "    observation_space,\n",
    "    4,\n",
    "    128,\n",
    "    action_space).cuda()\n",
    "\n",
    "targetActor = copy.deepcopy(actor).cuda()\n",
    "targetCritic = copy.deepcopy(critic).cuda()\n",
    "\n",
    "\n",
    "actorOpt = Adam(actor.parameters(), lr=1e-4)\n",
    "criticOpt = Adam(critic.parameters(), lr=1e-3, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - 0.001) + param.data * 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:18<00:00,  1.84s/it]\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm.tqdm(range(10)):\n",
    "    for s, a, r, s_, done in trainLoader:\n",
    "        ## Get data from memory\n",
    "        s, a, r, s_, done = s.cuda(), a.cuda(), r.cuda(), s_.cuda(), done.cuda()\n",
    "\n",
    "        ## Compute target\n",
    "        maxQ = targetCritic(s_.float(), targetActor(s_.float()).detach().float()).float()\n",
    "        target_q = r.float() + (1.0 - done.float()) * 0.90 * maxQ\n",
    "\n",
    "        ## Update the critic network\n",
    "        criticOpt.zero_grad()\n",
    "        current_q = critic(s.float(), a.float()).float()\n",
    "        criticLoss = F.smooth_l1_loss(current_q, target_q.detach().float()).float()\n",
    "        criticLoss.backward()\n",
    "        criticOpt.step()\n",
    "\n",
    "        ## Update the actor network\n",
    "        actorOpt.zero_grad()\n",
    "        actorLoss = -critic(s.float(), actor(s.float())).mean()\n",
    "        actorLoss.backward()\n",
    "        actorOpt.step()\n",
    "\n",
    "        ## Copy policy weights over to target net\n",
    "        soft_update(targetActor, actor)\n",
    "        soft_update(targetCritic, critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DistributedTSCSEnv(config['env_config'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.639162540435791\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
